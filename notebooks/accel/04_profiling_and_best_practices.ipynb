{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b010c34",
   "metadata": {},
   "source": [
    "# A Developer's Guide to Profiling and Best Practices in cuML\n",
    "\n",
    "This notebook is a practical guide for developers looking to optimize their GPU-accelerated machine learning workflows with `cuml` and RAPIDS. We'll follow a typical developer's journey: from initial benchmarking to deep profiling, troubleshooting common bottlenecks, and finally, summarizing the best practices for writing efficient code.\n",
    "\n",
    "This guide will cover:\n",
    "1.  **Benchmarking & Profiling**: Moving from simple timing to understanding *where* time is spent using `cProfile` and NVIDIA's Nsight Systems.\n",
    "2.  **Troubleshooting Common Bottlenecks**: Tackling the two most frequent issues: Out-of-Memory errors and slow data transfers between the CPU and GPU.\n",
    "3.  **A Summary of Best Practices**: A final checklist of principles for high-performance code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f462f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949cf05e",
   "metadata": {},
   "source": [
    "## Part 1: From Simple Timing to Deep Profiling\n",
    "\n",
    "You can't optimize what you can't measure. The first step in any optimization journey is to understand your code's performance.\n",
    "\n",
    "### Step 1: Basic Benchmarking with `time`\n",
    "\n",
    "The simplest method is to time your code's execution. It gives you a high-level baseline but doesn't tell you *why* it's slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gpu, _ = make_blobs(n_samples=100_000, n_features=50, random_state=42)\n",
    "model = NearestNeighbors(n_neighbors=10, algorithm='ivfflat')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(X_gpu)\n",
    "distances, indices = model.kneighbors(X_gpu)\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total execution time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6550e0c",
   "metadata": {},
   "source": [
    "### Step 2: Finding Python Bottlenecks with `cProfile`\n",
    "\n",
    "To understand which *functions* are taking the most time, we use a profiler. Python's built-in `cProfile` is an excellent first tool. It tracks every function call and tells you how much cumulative time was spent in each one, making it perfect for identifying slow spots in your Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_task_to_profile():\n",
    "    X_gpu, _ = make_blobs(n_samples=100_000, n_features=50, random_state=42)\n",
    "    model = NearestNeighbors(n_neighbors=10, algorithm='ivfflat')\n",
    "    model.fit(X_gpu)\n",
    "    distances, indices = model.kneighbors(X_gpu)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "# Run the profiler and save the stats to a file\n",
    "cProfile.run('knn_task_to_profile()', 'knn_profile_stats')\n",
    "\n",
    "# Load and print the stats, sorted by cumulative time\n",
    "p = pstats.Stats('knn_profile_stats')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293abf2",
   "metadata": {},
   "source": [
    "### Step 3: Deep GPU Analysis with NVIDIA Nsight Systems (`nsys`)\n",
    "\n",
    "`cProfile` shows you the Python world. To see what's happening on the GPU itself—CUDA kernel launches, memory copies, etc.—you need a system-level profiler. **NVIDIA Nsight Systems (`nsys`)** is the professional tool for this.\n",
    "\n",
    "The workflow involves running this command-line tool on a standalone script, which we create below for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a standalone Python script to be profiled by nsys\n",
    "script_content = \"\"\"\n",
    "import cupy as cp\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.datasets import make_blobs\n",
    "\n",
    "X_gpu, _ = make_blobs(n_samples=100_000, n_features=50, random_state=42)\n",
    "model = NearestNeighbors(n_neighbors=10, algorithm='ivfflat')\n",
    "model.fit(X_gpu)\n",
    "distances, indices = model.kneighbors(X_gpu)\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "print(\"Profiling script finished.\")\n",
    "\"\"\"\n",
    "\n",
    "with open('profile_script.py', 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "print(\"File 'profile_script.py' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74e494",
   "metadata": {},
   "source": [
    "#### How to Run `nsys`\n",
    "\n",
    "To generate a detailed report, you would run the following command in a **separate terminal** (with the conda environment activated). **Note: This requires the NVIDIA CUDA Toolkit to be installed.**\n",
    "\n",
    "```bash\n",
    "nsys profile python profile_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472e0dc",
   "metadata": {},
   "source": [
    "## Part 2: Troubleshooting the Most Common Bottlenecks\n",
    "\n",
    "Profiling often reveals two main culprits for poor performance in GPU data science: memory issues and data transfer overhead.\n",
    "\n",
    "### Bottleneck 1: The Memory Wall (Out-of-Memory Errors)\n",
    "\n",
    "GPUs have a fixed amount of VRAM. An \"Out of Memory\" (OOM) error is the most common issue you'll face.\n",
    "\n",
    "**Key Tactics for Memory Management:**\n",
    "1.  **Check Available Memory**: Always be aware of your memory budget.\n",
    "2.  **Use `float32`**: `float64` uses twice the memory and is rarely necessary for ML.\n",
    "3.  **Delete and Collect**: Actively delete large, unused objects and call the garbage collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check current memory\n",
    "free_mem, total_mem = cp.cuda.runtime.memGetInfo()\n",
    "print(f\"GPU Memory: {free_mem / 1e9:.2f} GB Free / {total_mem / 1e9:.2f} GB Total\")\n",
    "\n",
    "# 2. Attempt to create a large float64 array (this might fail on some GPUs)\n",
    "try:\n",
    "    large_arr64 = make_blobs(n_samples=10_000_000, n_features=10, dtype=np.float64)[0]\n",
    "    print(f\"float64 Array created, using {large_arr64.nbytes / 1e9:.2f} GB\")\n",
    "    del large_arr64\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create float64 array as expected: {e}\")\n",
    "\n",
    "\n",
    "# 3. Use float32 for efficiency and actively manage memory\n",
    "large_arr32 = make_blobs(n_samples=10_000_000, n_features=10, dtype=np.float32)[0]\n",
    "print(f\"float32 Array created, using {large_arr32.nbytes / 1e9:.2f} GB\")\n",
    "\n",
    "del large_arr32\n",
    "gc.collect()\n",
    "print(\"float32 array deleted and memory collected.\")\n",
    "\n",
    "free_mem_after, _ = cp.cuda.runtime.memGetInfo()\n",
    "print(f\"GPU Memory after cleanup: {free_mem_after / 1e9:.2f} GB Free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf35c26",
   "metadata": {},
   "source": [
    "### Bottleneck 2: The PCIe Bridge (CPU-GPU Data Transfers)\n",
    "\n",
    "Moving data between the CPU's RAM and the GPU's VRAM is slow. Unnecessary transfers will destroy your performance gains.\n",
    "\n",
    "> **The Golden Rule of RAPIDS:** Stay on the GPU. Only move data to the CPU (`.get()` or `.to_pandas()`) when you are completely finished with your computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be072ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gpu, _ = make_blobs(n_samples=5000, n_features=500, random_state=42)\n",
    "\n",
    "# BAD: Transferring data inside a loop\n",
    "start_time = time.time()\n",
    "max_values_bad = []\n",
    "for i in range(X_gpu.shape[1]):\n",
    "    col_cpu = X_gpu[:, i].get()  # SLOW: GPU -> CPU transfer in each iteration\n",
    "    max_values_bad.append(col_cpu.max())\n",
    "end_time = time.time()\n",
    "print(f\"BAD PRACTICE (transfer in loop): {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# GOOD: Compute on GPU, transfer only the final result\n",
    "start_time = time.time()\n",
    "max_values_gpu = X_gpu.max(axis=0)      # FAST: Stays on GPU\n",
    "max_values_good = max_values_gpu.get() # FAST: Transfer final small result array\n",
    "end_time = time.time()\n",
    "print(f\"GOOD PRACTICE (all on GPU):    {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b7dc2",
   "metadata": {},
   "source": [
    "## Part 3: A Checklist for Best Practices\n",
    "\n",
    "1.  **Measure First**: Use `time` for quick checks and profilers (`cProfile`, `nsys`) to find real bottlenecks before optimizing.\n",
    "2.  **Stay on the GPU**: Adhere to the Golden Rule. Minimize data transfers between CPU and GPU.\n",
    "3.  **Be Memory-Conscious**: Prefer `float32` precision. Actively `del` large objects and call `gc.collect()`.\n",
    "4.  **Choose the Right Algorithm**: The performance difference between `brute` and `ivfflat` is a perfect example. Understand your model's parameters to make informed choices.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This guide has walked through a developer's journey of optimizing a `cuml` workflow. By understanding how to profile, troubleshoot common errors, and apply best practices, you can build highly performant, GPU-accelerated machine learning pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
