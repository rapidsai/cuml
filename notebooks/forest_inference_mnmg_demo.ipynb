{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4284d3ef-9a67-4074-bf0f-17e8ab63370b",
   "metadata": {},
   "source": [
    "# Using Forest Inference Library (FIL) with multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537986d-28b7-4fa2-bf1d-af993acad69c",
   "metadata": {},
   "source": [
    "See [Forest Inference Library demo](./forest_inference_demo.ipynb) for a basic introduction to Forest Inference Library (FIL). In this notebook, we will show how to use FIL to run inference with tree models using multiple GPUs.\n",
    "\n",
    "We will FIL and Dask together. \n",
    "\n",
    "Below we will:\n",
    "1. **Create a Dask cluster** with multiple workers, where each worker is assigned a single GPU;\n",
    "\n",
    "2. **Generate synthetic data** and partition it evenly among the workers;\n",
    "    \n",
    "3. **Load FIL model** on each worker; and\n",
    "\n",
    "4. **Run parallel FIL .predict()** on each worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101408a6-44cf-4639-ad31-fe24f505bcbc",
   "metadata": {},
   "source": [
    "*Optional Kernel Restart*\n",
    "\n",
    "```python\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992a1dd-3da4-48e5-8435-cf5bbcfce0b5",
   "metadata": {},
   "source": [
    "## Dask imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69399775-34ab-4532-ab8f-50d9e91af661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from distributed import Client, wait, get_worker\n",
    "\n",
    "import dask.dataframe\n",
    "import dask.array\n",
    "import dask_cudf\n",
    "\n",
    "from cuml import ForestInference\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2427d6-e7e4-4b21-a554-6036b96a5de9",
   "metadata": {},
   "source": [
    "## Create a LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9964f-53b5-4127-848e-bf4bd7228845",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "workers = client.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_partitions = n_workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fdead-2141-47a3-8402-34bc52a70776",
   "metadata": {},
   "source": [
    "## Define size of synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f6eba-b656-4441-b171-198f9882214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1_000_000\n",
    "cols = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590651b-6be8-4a4f-a812-135db48d5896",
   "metadata": {},
   "source": [
    "## Generate synthetic query/inference data\n",
    "\n",
    "We will generate data on the CPU as a Dask array, then move it into GPU memory as a `dask.dataframe`, and ultimately convert it into a `dask_cudf.dataframe` so that it can be used in FIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07344c-17eb-4d7e-87db-213b752a46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dask.array.random.random(\n",
    "    size=(rows, cols),\n",
    "    chunks=(rows//n_partitions, cols)\n",
    ").astype(\"float32\")\n",
    "\n",
    "df = dask.dataframe.from_array(x).to_backend(\"cudf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf28c1-d31f-4222-b489-0e222fe606ed",
   "metadata": {},
   "source": [
    "## Persist data in GPU memory\n",
    "\n",
    "We can optionally persist our generated data (see [Persist documentation](https://docs.dask.org/en/latest/dataframe-best-practices.html?highlight=persist#persist-intelligently)), so that our lazy dataframe starts to be executed and saved in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910d998-bb0b-4cb0-8645-c4ceb21ceb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()\n",
    "wait(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff1c6b-0b9f-4161-a5b2-d0a208896b62",
   "metadata": {},
   "source": [
    "## Pre-load FIL model on each worker\n",
    "\n",
    "Before we run inference on our distributed dataset let's first load the tree model onto each worker. **Make sure to run [Forest Inference Library demo](./forest_inference_demo.ipynb) first to obtain the `xgb.model` file.**\n",
    "\n",
    "Here we'll leverage the worker's local storage which will persist after the function/task completes.\n",
    "\n",
    "For more see the [Dask worker documentation on storage](https://distributed.dask.org/en/latest/worker.html#storing-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4d8d8-5820-4254-8e86-281353da2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init(dask_worker, model_file=\"xgb.model\"):\n",
    "   dask_worker.data[\"fil_model\"] = ForestInference.load(\n",
    "       model_file,\n",
    "       layout=\"depth_first\",\n",
    "       is_classifier=True,\n",
    "       model_type=\"xgboost_ubj\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5521934-a36e-40c5-b507-0a9029fdafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "client.run(worker_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47364d4-5354-417e-8a78-a47ba7d76fa7",
   "metadata": {},
   "source": [
    "## Distributed FIL Predict on persisted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ccaf12-4453-4fbb-bfa3-330cb17e3b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_df):\n",
    "   worker = get_worker()\n",
    "   return worker.data[\"fil_model\"].predict(input_df, threshold=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82594e02-6e93-4355-af7c-042965397fab",
   "metadata": {},
   "source": [
    "Let's map the `predict` call to each of our partitions (i.e., the `dask_cudf.dataframe` chunks that we distributed among the workers )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7bf88-7f86-4ce3-ac41-4f2ed213e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_predictions = df.map_partitions(predict, meta=(\"predict\", int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed325b7-b9f9-4a43-a02a-a8908280f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "distributed_predictions.compute()\n",
    "toc = time.perf_counter()\n",
    "\n",
    "fil_inference_time = toc - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78832235-19bd-4dbf-93b6-3c627b1280f8",
   "metadata": {},
   "source": [
    "## Summarize the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68477de2-69d9-48d1-b981-81c51e0e9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(df)\n",
    "print(f\" {total_samples:,} inferences in {fil_inference_time:.5f} seconds\"\n",
    "      f\" -- {int(total_samples/fil_inference_time):,} inferences per second \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
