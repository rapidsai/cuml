{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuML on GPU and CPU\n",
    "\n",
    "cuML is a Scikit-learn based suite of fast, GPU-accelerated machine learning algorithms designed for data science and analytical tasks. Starting with version 23.10, cuML can also be run on CPU systems, increasing the ease of use (without code changes) in the following manners: \n",
    "\n",
    "- Allow users to prototype in systems without GPUs. \n",
    "- Allow library integrations without the need of dispatching and boilerplate code. \n",
    "- Allow users to train on one type of system and infer with the other in a subset of estimators (that will grow with each version). \n",
    "- Provide compatibility with the GPU/CPU open source pydata ecosystem. (brief cudf accel mention later) \n",
    "\n",
    "The majority of estimators of cuML can run in both CPU and GPU systems, with a subset of them allowing exporting models between GPU and CPU systems:\n",
    "\n",
    "[table to be inserted]\n",
    "\n",
    "This allows the same code to be guaranteed to run in both GPU and CPU systems: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m \n\u001b[1;32m      3\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      4\u001b[0m n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuml'"
     ]
    }
   ],
   "source": [
    "import cuml \n",
    "\n",
    "n_samples = 2**20\n",
    "n_features = 399\n",
    "\n",
    "random_state = 23\n",
    "\n",
    "#todo: change to sklearn dataset\n",
    "X, y = make_regression(n_samples=n_samples, \n",
    "                       n_features=n_features, \n",
    "                       random_state=random_state)\n",
    "\n",
    "#todo: mention briefly xdf in ending and link\n",
    "X = pandas.DataFrame(X)\n",
    "y = pandas.DataFrame(y)[0]\n",
    "\n",
    "X_cudf, X_cudf_test, y_cudf, y_cudf_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            test_size = 0.2, \n",
    "                                                            random_state=random_state)\n",
    "\n",
    "# is OLS the best example? \n",
    "ols_cuml = cuLinearRegression(fit_intercept=True,\n",
    "                              algorithm='eig')\n",
    "\n",
    "ols_cuml.fit(X_cudf, y_cudf)\n",
    "predict_cuml = ols_cuml.predict(X_cudf_test)\n",
    "\n",
    "print(predict_cuml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "For GPU systems, cuML still follows the [RAPIDS requirements] and nothing has changed for installing it. The cuML package and wheels are universal and can run in both GPU and CPU modes. For installing in CPU systems, similar to other packages it can be installed from conda/mamba with:\n",
    "\n",
    "```bash\n",
    "mamba install -c rapidsai cuml-cpu\n",
    "```\n",
    "\n",
    "This will install the CPU package (that doesn't require CUDA or GPUs in general), but usage and imports don't need to change. \n",
    "pip wheels 23.12\n",
    "\n",
    "\n",
    "This package can run most of the cuML estimators,as mentioned in the section above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Execution Platform\n",
    "\n",
    "Additionally to allowing the same code to be run in CPU systems, users can control which device exedcutes parts of the code. So in addition to the first example that can just be run in a CPU system with `cuml-cpu`, a system with the full cuML can execute in CPU mode as well. \n",
    "\n",
    "First we get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.datasets import make_regression, make_blobs\n",
    "from cuml.model_selection import train_test_split\n",
    "\n",
    "# todo: use sklearn.get_dataset\n",
    "X_blobs, y_blobs = make_blobs(n_samples=2000, \n",
    "                              n_features=20)\n",
    "X_train_blobs, X_test_blobs, y_train_blobs, y_test_blobs = train_test_split(X_blobs, \n",
    "                                                                            y_blobs, \n",
    "                                                                            test_size=0.2, shuffle=True)\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=2000, \n",
    "                               n_features=20)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_tes_reg = train_test_split(X_reg, \n",
    "                                                                   y_reg, \n",
    "                                                                   test_size=0.2, \n",
    "                                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't have a GPU with enough memory at your disposal at the moment? You can work on prototyping and run estimators in CPU-mode: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors()\n",
    "with using_device_type('cpu'):\n",
    "    nn.fit(X_train_blobs)\n",
    "    nearest_neighbors = nn.kneighbors(X_test_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to train your estimator with a special feature or hyperparameter only available in the paired CPU library? Initialize the cuML model with it and train on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.manifold import UMAP\n",
    "\n",
    "umap_model = UMAP(angular_rp_forest=True) # `angular_rp_forest` hyperparameter only available in UMAP library\n",
    "with using_device_type('cpu'):\n",
    "    umap_model.fit(X_train_blobs) # will run the UMAP library with the hyperparameter\n",
    "with using_device_type('gpu'):\n",
    "    transformed = umap_model.transform(X_test_blobs) # will run the cuML implementation of UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mechanisms to control execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU/device is the default execution platform :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.common.device_selection import using_device_type\n",
    "from cuml.common.device_selection import set_global_device_type, get_global_device_type\n",
    "\n",
    "initial_device_type = get_global_device_type()\n",
    "print('default execution device:', initial_device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators trainings and inferences inside a `using_device_type` context will be executed according to the execution platform selected :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in ['cpu', 'host', 'gpu', 'device']:\n",
    "    with using_device_type(param):\n",
    "        print('using_device_type({}):'.format(param), get_global_device_type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution platform can also be set at the global level from the `set_global_device_type` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_device_type('gpu')\n",
    "print('new device type:', get_global_device_type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross Device Training and Inference. \n",
    "\n",
    "While ML training workflows almost always benefit from the superior speed of GPUs, small-scale applications with limited traffic and loose latency requirements may be able to perform inference on CPU. Please note that this feature would only work with models implementing pickle serialization and GPU to CPU transfers.\n",
    "\n",
    "To train a model on GPU but deploy it on CPU : first, train the estimator on device and save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "with using_device_type('gpu'):\n",
    "    lin_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "pickle.dump(lin_reg, open(\"lin_reg.pkl\", \"wb\"))\n",
    "del lin_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, on the server, recover the estimator and run the inference on host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_lin_reg = pickle.load(open(\"lin_reg.pkl\", \"rb\"))\n",
    "with using_device_type('cpu'):\n",
    "    predictions = recovered_lin_reg.predict(X_test_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35840739db47a5016f18b089945bf3e154a2dca6d71cfb13687d370b69a146e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
