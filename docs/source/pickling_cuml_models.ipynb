{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling Models for Persistence\n",
    "\n",
    "This notebook demonstrates simple pickling of both single-GPU and multi-GPU cuML models for persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU Model Pickling\n",
    "\n",
    "All single-GPU estimators are pickleable. The following example demonstrates the creation of a synthetic dataset, training, and pickling of the resulting model for storage. Trained single-GPU models can also be used to distribute the inference on a Dask cluster, which the [Distributed Model Pickling](#distributed-model-pickling) section below demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic dataset for clustering\n",
    "X, y = make_blobs(\n",
    "    n_samples=50,\n",
    "    n_features=10,\n",
    "    centers=5,\n",
    "    cluster_std=0.4,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as cuKMeans\n",
    "\n",
    "# Initialize and fit KMeans model\n",
    "kmeans = cuKMeans(n_clusters=5).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend to use Pickle protocol 5 as it is more efficient especially for large arrays (models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the fitted model to disk\n",
    "pickle.dump(kmeans, open(\"kmeans_model.pkl\", \"wb\"), protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk\n",
    "kmeans_loaded_model = pickle.load(open(\"kmeans_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded model's cluster centers\n",
    "kmeans_loaded_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using joblib for Model Serialization\n",
    "\n",
    "joblib is a popular alternative to pickle for serializing machine learning models, offering better performance and compression for large NumPy arrays. It's particularly well-suited for cuML models with many parameters or large datasets. joblib provides efficient memory mapping and faster serialization compared to pickle for ML workloads.\n",
    "\n",
    "\n",
    "Note that pickle and joblib are often directly compatible, but we do not recommend to rely on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "kmeans_loaded_model = joblib.load(\"kmeans_model.pkl\")\n",
    "kmeans_loaded_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Model Pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with distributed cuML models using Dask, the distributed estimator wrappers in `cuml.dask` are not designed to be pickled directly. Instead, cuML provides a specialized workflow for persisting distributed models:\n",
    "\n",
    "1. **Extract the combined model**: After training a distributed model, use the `get_combined_model()` method to extract a single-GPU version of the trained model\n",
    "2. **Serialize the combined model**: The extracted single-GPU model can be pickled or saved using pickle or joblib, just like any other cuML model\n",
    "3. **Flexible inference**: The saved model can be used for inference in multiple ways:\n",
    "   - **Single-GPU inference**: Load the model directly for single-GPU predictions\n",
    "   - **Distributed inference**: Use the `ParallelPostFit` wrapper from [Dask-ML](https://ml.dask.org/meta-estimators.html) to distribute inference across a Dask cluster\n",
    "\n",
    "This approach allows you to choose the right amount of resources for both training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "# Set up Dask cluster\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.datasets import make_blobs\n",
    "\n",
    "# Get number of workers\n",
    "n_workers = len(client.scheduler_info()[\"workers\"].keys())\n",
    "\n",
    "# Generate distributed dataset\n",
    "X, y = make_blobs(\n",
    "    n_samples=5000,\n",
    "    n_features=30,\n",
    "    centers=5,\n",
    "    cluster_std=0.4,\n",
    "    random_state=0,\n",
    "    n_parts=n_workers * 5\n",
    ")\n",
    "\n",
    "# Persist data in memory\n",
    "X = X.persist()\n",
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.cluster import KMeans as cuDaskKMeans\n",
    "\n",
    "# Initialize and train the distributed KMeans model\n",
    "distributed_kmeans = cuDaskKMeans(n_clusters=5).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract single-GPU model and save it\n",
    "combined_kmeans = distributed_kmeans.get_combined_model()\n",
    "pickle.dump(combined_kmeans, open(\"kmeans_model.pkl\", \"wb\"), protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the single-GPU model\n",
    "combined_kmeans_loaded_model = pickle.load(open(\"kmeans_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded model's cluster centers\n",
    "combined_kmeans_loaded_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Between cuML and scikit-learn Models\n",
    "\n",
    "Many cuML estimators provide an `as_sklearn()` / `from_sklearn` methods that enable direct conversion to and from native scikit-learn estimators. This functionality is particularly valuable for serialization scenarios requiring maximum compatibility, as well as for hybrid deployment workflows where you can train models on GPU-accelerated systems and then run inference on CPU-only environments without needing to install cuML on the target machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as cuKMeans\n",
    "from cuml.datasets import make_blobs\n",
    "from cuml.metrics.cluster import adjusted_rand_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic dataset for clustering\n",
    "X, y = make_blobs(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    centers=5,\n",
    "    cluster_std=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train cuML KMeans\n",
    "kmeans = cuKMeans(n_clusters=5, random_state=42).fit(X)\n",
    "\n",
    "# Make predictions with cuML model\n",
    "cu_predictions = kmeans.predict(X)\n",
    "cu_ari_score = adjusted_rand_score(y, cu_predictions)\n",
    "print(f\"cuML KMeans ARI score: {cu_ari_score:.4f}\")\n",
    "print(f\"cuML KMeans cluster centers shape: {kmeans.cluster_centers_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cuML model to scikit-learn model\n",
    "kmeans_sklearn = kmeans.as_sklearn()\n",
    "print(f\"Converted to scikit-learn model: {type(kmeans_sklearn)}\")\n",
    "\n",
    "# Save scikit-learn model to disk\n",
    "pickle.dump(kmeans_sklearn, open(\"kmeans_model_sklearn.pkl\", \"wb\"), protocol=5)\n",
    "print(\"scikit-learn KMeans model saved with pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupy import asnumpy\n",
    "\n",
    "# Load scikit-learn model and verify prediction quality\n",
    "kmeans_loaded_sklearn = pickle.load(open(\"kmeans_model_sklearn.pkl\", \"rb\"))\n",
    "sklearn_predictions = kmeans_loaded_sklearn.predict(asnumpy(X))\n",
    "sklearn_ari_score = adjusted_rand_score(y, sklearn_predictions)\n",
    "print(f\"Loaded sklearn KMeans ARI score: {sklearn_ari_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also construct a cuML model from a scikit-learn model. This is especially useful if you are working with a pre-trained model and want to run faster inference on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-construct the cuML model from the scikit-learn model\n",
    "kmeans_from_sklearn = cuKMeans.from_sklearn(kmeans_loaded_sklearn)\n",
    "cu_predictions = kmeans_from_sklearn.predict(X)\n",
    "print(\"Re-constructed cuML KMeans ARI Score: \", adjusted_rand_score(y, cu_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting cuML Random Forest models for inferencing on machines without GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export cuML Random Forest models and run predictions on machines without NVIDIA GPUs using the [Treelite](https://github.com/dmlc/treelite) library. This enables you to train on GPU-accelerated systems and deploy on CPU-only infrastructure.\n",
    "\n",
    "### Export Process\n",
    "\n",
    "1. **Convert to Treelite format**: Use `as_treelite()` to transform your cuML Random Forest model\n",
    "2. **Serialize the model**: Call `.serialize()` to create a portable checkpoint file\n",
    "3. **Deploy anywhere**: Install Treelite on the target machine and load the model for inference\n",
    "\n",
    "Treelite provides optimized CPU inference with efficient serialization, making it ideal for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestClassifier as cuRandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = X.astype(np.float32), y.astype(np.int32)\n",
    "\n",
    "# Train Random Forest model\n",
    "random_forest = cuRandomForestClassifier(max_depth=3, random_state=0, n_estimators=10).fit(X, y)\n",
    "\n",
    "# Export cuML RF model as Treelite checkpoint\n",
    "treelite_checkpoint_path = \"./checkpoint.tl\"\n",
    "random_forest.as_treelite().serialize(treelite_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copy the generated checkpoint file `checkpoint.tl` to another machine on which you'd like to run predictions.\n",
    "\n",
    "3. On the target machine, install Treelite by running `pip install treelite` or `conda install -c conda-forge treelite`. The machine does not need to have NVIDIA GPUs and does not need to have cuML installed.\n",
    "\n",
    "4. You can now load the model from the checkpoint, by running the following on the target machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite\n",
    "\n",
    "# Load the Treelite model (checkpoint file has been copied over)\n",
    "treelite_checkpoint_path = \"./checkpoint.tl\"\n",
    "treelite_model = treelite.Model.deserialize(treelite_checkpoint_path)\n",
    "\n",
    "# Make predictions using Treelite\n",
    "predictions = treelite.gtil.predict(treelite_model, X, pred_margin=True)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbsphinx": {
   "execute": "never"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
