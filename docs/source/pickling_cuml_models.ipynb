{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling Models for Persistence\n",
    "\n",
    "This notebook demonstrates simple pickling of both single-GPU and multi-GPU cuML models for persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU Model Pickling\n",
    "\n",
    "All single-GPU estimators are pickleable. The following example demonstrates the creation of a synthetic dataset, training, and pickling of the resulting model for storage. Trained single-GPU models can also be used to distribute the inference on a Dask cluster, which the `Distributed Model Pickling` section below demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic dataset for clustering\n",
    "X, y = make_blobs(\n",
    "    n_samples=50,\n",
    "    n_features=10,\n",
    "    centers=5,\n",
    "    cluster_std=0.4,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as cuKMeans\n",
    "\n",
    "# Initialize and fit KMeans model\n",
    "kmeans_model = cuKMeans(n_clusters=5)\n",
    "kmeans_fitted = kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend to use Pickle protocol 5 as it is more efficient especially for large arrays (models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the fitted model to disk\n",
    "pickle.dump(kmeans_fitted, open(\"kmeans_model.pkl\", \"wb\"), protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk\n",
    "kmeans_loaded = pickle.load(open(\"kmeans_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded model's cluster centers\n",
    "kmeans_loaded.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using joblib for Model Serialization\n",
    "\n",
    "joblib is a popular alternative to pickle for serializing machine learning models, offering better performance and compression for large NumPy arrays. It's particularly well-suited for cuML models with many parameters or large datasets. joblib provides efficient memory mapping and faster serialization compared to pickle for ML workloads.\n",
    "\n",
    "\n",
    "Note that pickle and joblib are often directly compatible, but we do not recommend to rely on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "kmeans_loaded = joblib.load(\"kmeans_model.pkl\")\n",
    "kmeans_loaded.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Model Pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with distributed cuML models using Dask, the distributed estimator wrappers in `cuml.dask` are not designed to be pickled directly. Instead, cuML provides a specialized workflow for persisting distributed models:\n",
    "\n",
    "1. **Extract the combined model**: After training a distributed model, use the `get_combined_model()` method to extract a single-GPU version of the trained model\n",
    "2. **Serialize the combined model**: The extracted single-GPU model can be pickled or saved using pickle or joblib, just like any other cuML model\n",
    "3. **Flexible inference**: The saved model can be used for inference in multiple ways:\n",
    "   - **Single-GPU inference**: Load the model directly for single-GPU predictions\n",
    "   - **Distributed inference**: Use the `ParallelPostFit` wrapper from [Dask-ML](https://ml.dask.org/meta-estimators.html) to distribute inference across a Dask cluster\n",
    "\n",
    "This approach allows you to choose the right amount of resources for both training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "# Set up Dask cluster\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.datasets import make_blobs\n",
    "\n",
    "# Get number of workers\n",
    "n_workers = len(client.scheduler_info()[\"workers\"].keys())\n",
    "\n",
    "# Generate distributed dataset\n",
    "X, y = make_blobs(\n",
    "    n_samples=5000,\n",
    "    n_features=30,\n",
    "    centers=5,\n",
    "    cluster_std=0.4,\n",
    "    random_state=0,\n",
    "    n_parts=n_workers * 5\n",
    ")\n",
    "\n",
    "# Persist data in memory\n",
    "X = X.persist()\n",
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.cluster import KMeans as cuDaskKMeans\n",
    "\n",
    "# Initialize distributed KMeans model\n",
    "dask_kmeans_model = cuDaskKMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the distributed model\n",
    "# dask_kmeans_fitted = dask_kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Extract single-GPU model and save it\n",
    "# single_gpu_kmeans = dask_kmeans_fitted.get_combined_model()\n",
    "# pickle.dump(single_gpu_kmeans, open(\"kmeans_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the single-GPU model\n",
    "# single_gpu_kmeans_loaded = pickle.load(open(\"kmeans_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded model's cluster centers\n",
    "# single_gpu_kmeans_loaded.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded model's cluster centers\n",
    "kmeans_loaded.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting cuML Random Forest models for inferencing on machines without GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export cuML Random Forest models and run predictions on machines without NVIDIA GPUs using the [Treelite](https://github.com/dmlc/treelite) library. This enables you to train on GPU-accelerated systems and deploy on CPU-only infrastructure.\n",
    "\n",
    "### Export Process\n",
    "\n",
    "1. **Convert to Treelite format**: Use `as_treelite()` to transform your cuML Random Forest model\n",
    "2. **Serialize the model**: Call `.serialize()` to create a portable checkpoint file\n",
    "3. **Deploy anywhere**: Install Treelite on the target machine and load the model for inference\n",
    "\n",
    "Treelite provides optimized CPU inference with efficient serialization, making it ideal for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = X.astype(np.float32), y.astype(np.int32)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = cuRF(max_depth=3, random_state=0, n_estimators=10)\n",
    "rf_fitted = rf_model.fit(X, y)\n",
    "\n",
    "# Export cuML RF model as Treelite checkpoint\n",
    "rf_fitted.as_treelite().serialize(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copy the generated checkpoint file `checkpoint.tl` to another machine on which you'd like to run predictions.\n",
    "\n",
    "3. On the target machine, install Treelite by running `pip install treelite` or `conda install -c conda-forge treelite`. The machine does not need to have NVIDIA GPUs and does not need to have cuML installed.\n",
    "\n",
    "4. You can now load the model from the checkpoint, by running the following on the target machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite\n",
    "\n",
    "# Load the Treelite model (checkpoint file has been copied over)\n",
    "checkpoint_path = \"./checkpoint.tl\"\n",
    "tl_model = treelite.Model.deserialize(checkpoint_path)\n",
    "\n",
    "# Make predictions using Treelite\n",
    "out_prob = treelite.gtil.predict(tl_model, X, pred_margin=True)\n",
    "print(out_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbsphinx": {
   "execute": "never"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
