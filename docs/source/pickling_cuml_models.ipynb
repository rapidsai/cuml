{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serialization and Persistence\n",
    "\n",
    "This notebook demonstrates how to save and load cuML models using various serialization methods, including pickle, joblib, and cross-platform deployment strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU Model Serialization\n",
    "\n",
    "All single-GPU cuML estimators support serialization using standard Python libraries. This section demonstrates:\n",
    "\n",
    "1. **Training a model** on synthetic data\n",
    "2. **Saving the model** using pickle and joblib\n",
    "3. **Loading the model** for future use\n",
    "\n",
    "Trained single-GPU models can also be used for distributed inference on Dask clusters, as shown in the [Distributed Model Serialization](#distributed-model-serialization) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans\n",
    "from cuml.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic dataset for clustering\n",
    "X, y = make_blobs(\n",
    "    n_samples=50, n_features=10, centers=5, cluster_std=0.4, random_state=0\n",
    ")\n",
    "# Initialize and fit KMeans model\n",
    "kmeans = KMeans(n_clusters=5).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation:** Use Pickle protocol 5 for better performance with large arrays and models. Protocol 5 provides significant speed improvements for NumPy arrays and cuML models with large parameter sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the fitted model to disk\n",
    "with open(\"kmeans_model.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(kmeans, output_file, protocol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** The model can be restored using pickle, but requires the same cuML version used for training. If you need to load models across different cuML versions, consider using the [scikit-learn conversion](#converting-between-cuml-and-scikit-learn-models) approach instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk\n",
    "with open(\"kmeans_model.pkl\", \"rb\") as input_file:\n",
    "    kmeans_loaded_model = pickle.load(input_file)\n",
    "\n",
    "# Display the loaded model's cluster centers\n",
    "kmeans_loaded_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using joblib for Model Serialization\n",
    "\n",
    "joblib is an optimized alternative to pickle for machine learning models, offering:\n",
    "\n",
    "- **Better performance** for large NumPy arrays and cuML models\n",
    "- **Efficient compression** for models with many parameters\n",
    "- **Memory mapping** for faster loading of large models\n",
    "- **Optimized serialization** specifically designed for ML workloads\n",
    "\n",
    "**Note:** While pickle and joblib files are often compatible, we recommend using the same library for both saving and loading to ensure reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(kmeans, \"kmeans_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then reload the model with joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_loaded_model = joblib.load(\"kmeans_model.joblib\")\n",
    "kmeans_loaded_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Model Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with distributed cuML models using Dask, the distributed estimator wrappers in `cuml.dask` are not designed to be pickled directly. Instead, cuML provides a specialized workflow:\n",
    "\n",
    "### Workflow Steps\n",
    "\n",
    "1. **Extract the combined model**: Use `get_combined_model()` to extract a single-GPU version of the trained distributed model\n",
    "2. **Serialize the combined model**: Save the extracted model using pickle or joblib (same as any cuML model)\n",
    "3. **Flexible inference**: Use the saved model in multiple ways:\n",
    "   - **Single-GPU inference**: Load directly for single-GPU predictions\n",
    "   - **Distributed inference**: Use `ParallelPostFit` from [Dask-ML](https://ml.dask.org/meta-estimators.html) to distribute inference across a Dask cluster\n",
    "\n",
    "This approach allows you to choose the optimal resources for both training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "# Set up Dask cluster\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.datasets import make_blobs\n",
    "from cuml.dask.cluster import KMeans as DistributedKMeans\n",
    "\n",
    "# Get number of workers\n",
    "n_workers = client.scheduler_info()[\"n_workers\"]\n",
    "\n",
    "# Generate distributed dataset\n",
    "X, y = make_blobs(\n",
    "    n_samples=5000,\n",
    "    n_features=30,\n",
    "    centers=5,\n",
    "    cluster_std=0.4,\n",
    "    random_state=0,\n",
    "    # 5 parts per worker to demonstrate distributed inference\n",
    "    n_parts=n_workers * 5, \n",
    ")\n",
    "\n",
    "# Initialize and train the distributed KMeans model\n",
    "distributed_kmeans = DistributedKMeans(n_clusters=5).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save it with pickle like before, but we have to _combine_ it into a non-distributed model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract single-GPU model and save it\n",
    "combined_kmeans = distributed_kmeans.get_combined_model()\n",
    "\n",
    "with open(\"kmeans_model.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(combined_kmeans, output_file, protocol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can reload this model just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the single-GPU model\n",
    "with open(\"kmeans_model.pkl\", \"rb\") as input_file:\n",
    "    combined_kmeans_loaded_model = pickle.load(input_file)\n",
    "\n",
    "# Display the first 3 rows of the loaded model's cluster centers\n",
    "combined_kmeans_loaded_model.cluster_centers_[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Between cuML and scikit-learn Models\n",
    "\n",
    "Many cuML estimators provide `as_sklearn()` and `from_sklearn()` methods for seamless conversion between cuML and scikit-learn formats.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Cross-platform deployment**: Train on GPU systems, deploy on CPU-only machines\n",
    "- **Maximum compatibility**: Use standard scikit-learn serialization tools\n",
    "- **Hybrid workflows**: Mix cuML and scikit-learn in the same pipeline\n",
    "- **Legacy integration**: Convert existing scikit-learn models to cuML for GPU acceleration\n",
    "\n",
    "This approach eliminates the need to install cuML on deployment machines while maintaining model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from cuml.cluster import KMeans\n",
    "from cuml.datasets import make_blobs\n",
    "from cuml.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "# Generate synthetic dataset for clustering\n",
    "X, y = make_blobs(\n",
    "    n_samples=1000, n_features=20, centers=5, cluster_std=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Train cuML KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(X)\n",
    "\n",
    "# Make predictions with cuML model\n",
    "predictions = kmeans.predict(X)\n",
    "score = adjusted_rand_score(y, predictions)\n",
    "print(f\"cuML KMeans ARI score: {score:.4f}\")\n",
    "print(f\"cuML KMeans cluster centers shape: {kmeans.cluster_centers_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert this cuML model into a native scikit-learn estimator using the `as_sklearn()` method. This enables standard scikit-learn serialization and deployment on any Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cuML model to scikit-learn model\n",
    "kmeans_sklearn = kmeans.as_sklearn()\n",
    "print(f\"Converted to scikit-learn model: {type(kmeans_sklearn)}\")\n",
    "\n",
    "# Save scikit-learn model to disk\n",
    "pickle.dump(kmeans_sklearn, open(\"kmeans_model_sklearn.pkl\", \"wb\"), protocol=5)\n",
    "print(\"scikit-learn KMeans model saved with pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickled scikit-learn model can be loaded and executed on any Python environment with only scikit-learn installed â€“ no cuML or GPU required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupy import asnumpy\n",
    "\n",
    "# Load scikit-learn model and verify prediction quality\n",
    "kmeans_loaded_sklearn = pickle.load(open(\"kmeans_model_sklearn.pkl\", \"rb\"))\n",
    "sklearn_predictions = kmeans_loaded_sklearn.predict(asnumpy(X))\n",
    "sklearn_score = adjusted_rand_score(y, sklearn_predictions)\n",
    "print(f\"Loaded sklearn KMeans ARI score: {sklearn_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also reconstruct a cuML model from a scikit-learn model using `from_sklearn()`. This is particularly useful for:\n",
    "\n",
    "- **Pre-trained models**: Convert existing scikit-learn models for GPU acceleration\n",
    "- **Performance optimization**: Run faster inference on GPU hardware\n",
    "- **Hybrid workflows**: Switch between CPU and GPU execution as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-construct the cuML model from the scikit-learn model\n",
    "kmeans_from_sklearn = KMeans.from_sklearn(kmeans_loaded_sklearn)\n",
    "predictions = kmeans_from_sklearn.predict(X)\n",
    "print(\"Re-constructed cuML KMeans ARI Score: \", adjusted_rand_score(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Random Forest Models for CPU-Only Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export cuML Random Forest models for deployment on machines without NVIDIA GPUs using the [Treelite](https://github.com/dmlc/treelite) library.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **CPU-only deployment**: Run trained models on any machine\n",
    "- **Optimized inference**: Treelite provides highly optimized CPU inference\n",
    "- **Small footprint**: No cuML or GPU dependencies required\n",
    "- **Production ready**: Efficient serialization and fast loading\n",
    "\n",
    "### Export Process\n",
    "\n",
    "1. **Convert to Treelite format**: Use `as_treelite()` to transform your cuML Random Forest model\n",
    "2. **Serialize the model**: Call `.serialize()` to create a portable checkpoint file\n",
    "3. **Deploy anywhere**: Install Treelite on the target machine and load the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load and prepare iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = X.astype(np.float32), y.astype(np.int32)\n",
    "\n",
    "# Train Random Forest model\n",
    "random_forest = RandomForestClassifier(\n",
    "    max_depth=3, random_state=0, n_estimators=10\n",
    ").fit(X, y)\n",
    "\n",
    "# Export cuML RF model as Treelite checkpoint\n",
    "treelite_checkpoint_path = \"./checkpoint.tl\"\n",
    "random_forest.as_treelite().serialize(treelite_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Steps\n",
    "\n",
    "1. **Copy the checkpoint file**: Transfer `checkpoint.tl` to your target machine\n",
    "2. **Install Treelite**: Run `pip install treelite` or `conda install -c conda-forge treelite`\n",
    "   - No NVIDIA GPUs required\n",
    "   - No cuML installation needed\n",
    "3. **Load and use the model**: Run the code below on the target machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite\n",
    "\n",
    "# Load the Treelite model (checkpoint file has been copied over)\n",
    "treelite_checkpoint_path = \"./checkpoint.tl\"\n",
    "treelite_model = treelite.Model.deserialize(treelite_checkpoint_path)\n",
    "\n",
    "# Make predictions using Treelite\n",
    "predictions = treelite.gtil.predict(treelite_model, X, pred_margin=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10-cu13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
