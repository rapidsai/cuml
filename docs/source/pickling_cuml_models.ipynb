{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling Models for Persistence\n",
    "\n",
    "This notebook demonstrates simple pickling of both single-GPU and multi-GPU cuML models for persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU Model Pickling\n",
    "\n",
    "All single-GPU estimators are pickleable. The following example demonstrates the creation of a synthetic dataset, training, and pickling of the resulting model for storage. Trained single-GPU models can also be used to distribute the inference on a Dask cluster, which the `Distributed Model Pickling` section below demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic dataset for clustering\n",
    "X, y = make_blobs(\n",
    "    n_samples=50,\n",
    "    n_features=10,\n",
    "    centers=5,\n",
    "    cluster_std=0.4,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as cuKMeans\n",
    "\n",
    "# Initialize and fit KMeans model\n",
    "kmeans_model = cuKMeans(n_clusters=5)\n",
    "kmeans_fitted = kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend to use Pickle protocol 5 as it is more efficient especially for large arrays (models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the fitted model to disk\n",
    "pickle.dump(kmeans_fitted, open(\"kmeans_model.pkl\", \"wb\"), protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk\n",
    "kmeans_loaded = pickle.load(open(\"kmeans_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded model's cluster centers\n",
    "kmeans_loaded.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using joblib for Model Serialization\n",
    "\n",
    "joblib is another popular library for serializing Python objects, particularly NumPy arrays and scikit-learn models. It's often faster than pickle for large arrays and provides better compression. cuML models can also be serialized using joblib.\n",
    "\n",
    "The following example demonstrates how to use joblib to save and load cuML models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.datasets.classification import make_classification\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic dataset for classification\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Random Forest model\n",
    "rf_model = cuRF(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_fitted = rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = rf_fitted.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted model using joblib\n",
    "model_filename = \"rf_model.joblib\"\n",
    "dump(rf_fitted, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk using joblib\n",
    "loaded_model = load(model_filename)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Verify the loaded model works by making predictions\n",
    "loaded_predictions = loaded_model.predict(X_test)\n",
    "loaded_accuracy = accuracy_score(y_test, loaded_predictions)\n",
    "print(f\"Loaded model accuracy: {loaded_accuracy:.4f}\")\n",
    "\n",
    "# Verify predictions are identical\n",
    "predictions_match = np.array_equal(predictions, loaded_predictions)\n",
    "print(f\"Predictions match: {predictions_match}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of joblib over pickle\n",
    "\n",
    "- **Better compression**: joblib uses more efficient compression algorithms, resulting in smaller file sizes\n",
    "- **Faster serialization**: Particularly efficient for large NumPy arrays and scikit-learn-style models\n",
    "- **Memory efficient**: Can handle large objects that might cause memory issues with pickle\n",
    "- **Cross-platform compatibility**: Works well across different operating systems and Python versions\n",
    "\n",
    "### When to use joblib vs pickle\n",
    "\n",
    "- Use **joblib** when working with machine learning models, especially those with large arrays (like Random Forest models with many trees)\n",
    "- Use **pickle** for general Python object serialization or when you need maximum compatibility\n",
    "- Both work well with cuML models, so the choice often comes down to personal preference and specific use case requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Model Pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed estimator wrappers inside of the `cuml.dask` are not intended to be pickled directly. The Dask cuML estimators provide a function `get_combined_model()`, which returns the trained single-GPU model for pickling. The combined model can be used for inference on a single-GPU, and the `ParallelPostFit` wrapper from the [Dask-ML](https://ml.dask.org/meta-estimators.html) library can be used to perform distributed inference on a Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "# Set up Dask cluster\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.datasets import make_blobs\n",
    "\n",
    "# Get number of workers\n",
    "n_workers = len(client.scheduler_info()[\"workers\"].keys())\n",
    "\n",
    "# Generate distributed dataset\n",
    "X, y = make_blobs(\n",
    "    n_samples=5000,\n",
    "    n_features=30,\n",
    "    centers=5,\n",
    "    cluster_std=0.4,\n",
    "    random_state=0,\n",
    "    n_parts=n_workers * 5\n",
    ")\n",
    "\n",
    "# Persist data in memory\n",
    "X = X.persist()\n",
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.cluster import KMeans as cuDaskKMeans\n",
    "\n",
    "# Initialize distributed KMeans model\n",
    "dask_kmeans_model = cuDaskKMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the distributed model\n",
    "# dask_kmeans_fitted = dask_kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Extract single-GPU model and save it\n",
    "# single_gpu_kmeans = dask_kmeans_fitted.get_combined_model()\n",
    "# pickle.dump(single_gpu_kmeans, open(\"kmeans_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the single-GPU model\n",
    "# single_gpu_kmeans_loaded = pickle.load(open(\"kmeans_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded model's cluster centers\n",
    "# single_gpu_kmeans_loaded.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting cuML Random Forest models for inferencing on machines without GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with cuML version 21.06, you can export cuML Random Forest models and run predictions with them on machines without NVIDIA GPUs. The [Treelite](https://github.com/dmlc/treelite) package defines an efficient exchange format that lets you portably move the cuML Random Forest models to other machines. We will refer to the exchange format as 'checkpoints.'\n",
    "\n",
    "Here are the steps to export the model:\n",
    "\n",
    "1. Call `as_treelite().serialize()` to obtain the checkpoint file from the cuML Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = X.astype(np.float32), y.astype(np.int32)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = cuRF(max_depth=3, random_state=0, n_estimators=10)\n",
    "rf_fitted = rf_model.fit(X, y)\n",
    "\n",
    "# Export cuML RF model as Treelite checkpoint\n",
    "rf_fitted.as_treelite().serialize(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copy the generated checkpoint file `checkpoint.tl` to another machine on which you'd like to run predictions.\n",
    "\n",
    "3. On the target machine, install Treelite by running `pip install treelite` or `conda install -c conda-forge treelite`. The machine does not need to have NVIDIA GPUs and does not need to have cuML installed.\n",
    "\n",
    "4. You can now load the model from the checkpoint, by running the following on the target machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite\n",
    "\n",
    "# Load the Treelite model (checkpoint file has been copied over)\n",
    "checkpoint_path = \"./checkpoint.tl\"\n",
    "tl_model = treelite.Model.deserialize(checkpoint_path)\n",
    "\n",
    "# Make predictions using Treelite\n",
    "out_prob = treelite.gtil.predict(tl_model, X, pred_margin=True)\n",
    "print(out_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbsphinx": {
   "execute": "never"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
